{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T12:20:10.465396Z","iopub.status.busy":"2023-11-04T12:20:10.464994Z","iopub.status.idle":"2023-11-04T12:20:22.670564Z","shell.execute_reply":"2023-11-04T12:20:22.669338Z","shell.execute_reply.started":"2023-11-04T12:20:10.465363Z"},"id":"hLvatxY5Wqp8","outputId":"e9ffed7a-fef0-43e7-9adc-e2030654ee91","trusted":true},"outputs":[],"source":["!pip install gym gym[other] tensorflow keras autorom gym[accept-rom-license] gym[atari] torch"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T12:20:22.673681Z","iopub.status.busy":"2023-11-04T12:20:22.673289Z","iopub.status.idle":"2023-11-04T12:20:22.678717Z","shell.execute_reply":"2023-11-04T12:20:22.677771Z","shell.execute_reply.started":"2023-11-04T12:20:22.673641Z"},"id":"G1gQN86MWotC","trusted":true},"outputs":[],"source":["import numpy as np\n","from collections import deque\n","import gym\n","import random"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T12:20:22.680207Z","iopub.status.busy":"2023-11-04T12:20:22.679923Z","iopub.status.idle":"2023-11-04T12:20:22.874992Z","shell.execute_reply":"2023-11-04T12:20:22.874041Z","shell.execute_reply.started":"2023-11-04T12:20:22.680183Z"},"id":"a3TBeAMFWotH","outputId":"2edc9e00-7fc3-4fa1-8003-b0d62ecf8d37","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n","[Powered by Stella]\n","/home/meliioko/dqn-breakout/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (210, 160)\u001b[0m\n","  logger.warn(\n","/home/meliioko/dqn-breakout/.venv/lib/python3.10/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/meliioko/dqn-breakout/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n","  logger.warn(\n"]}],"source":["env = gym.make(\"Pong-v4\", obs_type='grayscale', render_mode='rgb_array', full_action_space=False)\n","env = gym.wrappers.AtariPreprocessing(env=env, frame_skip=1)\n","env = gym.wrappers.FrameStack(env=env, num_stack=4)\n","env = gym.wrappers.RecordVideo(env, 'videos', episode_trigger= lambda x : x % 30 == 0)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T12:20:22.877289Z","iopub.status.busy":"2023-11-04T12:20:22.876596Z","iopub.status.idle":"2023-11-04T12:20:22.889044Z","shell.execute_reply":"2023-11-04T12:20:22.888087Z","shell.execute_reply.started":"2023-11-04T12:20:22.877251Z"},"id":"DAxcMkx5gsh2","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","class DQN(nn.Module):\n","    def __init__(self, action_size):\n","        super(DQN, self).__init__()\n","        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)  # Assuming input_shape is (channels, height, width)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n","\n","        # Compute the size of the output of the last conv layer\n","        def conv2d_size_out(size, kernel_size=3, stride=1):\n","            return (size - (kernel_size - 1) - 1) // stride + 1\n","\n","        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(84, 8, 4), 4, 2), 3, 1)\n","        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(84, 8, 4), 4, 2), 3, 1)\n","        linear_input_size = convw * convh * 64\n","\n","        self.fc = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(linear_input_size, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, action_size)\n","        )\n","\n","    def forward(self, x):\n","        x = torch.relu(self.conv1(x))\n","        x = torch.relu(self.conv2(x))\n","        x = torch.relu(self.conv3(x))\n","        return self.fc(x)\n","\n","def update_target_network(target, source):\n","    target.load_state_dict(source.state_dict())\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T12:20:22.893786Z","iopub.status.busy":"2023-11-04T12:20:22.893235Z","iopub.status.idle":"2023-11-04T12:20:22.922299Z","shell.execute_reply":"2023-11-04T12:20:22.921464Z","shell.execute_reply.started":"2023-11-04T12:20:22.893751Z"},"id":"X-9rxdLWWotK","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using GPU: NVIDIA GeForce RTX 2060\n"]}],"source":["import copy\n","# Set parameters\n","N = 60000  # Replay memory capacity\n","M = 10000  # Number of episodes\n","T = 10000  # Max steps per episode\n","C = 40  # Target network update frequency\n","epsilon = 1\n","epsilon_decay = 0.99\n","epsilon_min = 0.1\n","gamma = 0.99\n","action_size = env.action_space.n  # Number of actions\n","state_size = env.observation_space.shape[0]  # State size\n","\n","# Initialize replay memory\n","\n","\n","Q = DQN(action_size)\n","Q_hat = copy.deepcopy(Q)\n","D = deque(maxlen=N)\n","\n","\n","# Check if a GPU is available\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Using CPU\")\n","\n","Q.to(device)\n","Q_hat.to(device)\n","optimizer = optim.Adam(Q.parameters(), lr=0.0025)\n","criterion = nn.MSELoss()\n","\n","\n","\n","# Convert numpy array to PyTorch tensor\n","def preprocess_state(state):\n","  return torch.tensor(np.asarray(state)).float().div(255).unsqueeze(0).to(device)  # Scales to [0,1]\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T12:25:19.997666Z","iopub.status.busy":"2023-11-04T12:25:19.997279Z","iopub.status.idle":"2023-11-04T12:27:05.158790Z","shell.execute_reply":"2023-11-04T12:27:05.157415Z","shell.execute_reply.started":"2023-11-04T12:25:19.997626Z"},"id":"Cf8kqTdyWotM","outputId":"77df7ede-d0e7-4470-a7a2-710ab31504ad","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/10000 [00:00<?, ?it/s]/home/meliioko/dqn-breakout/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n","  if not isinstance(terminated, (bool, np.bool8)):\n","/home/meliioko/dqn-breakout/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:289: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n","  logger.warn(\n","  0%|          | 0/10000 [00:01<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["Moviepy - Building video /home/meliioko/dqn-breakout/videos/rl-video-episode-0.mp4.\n","Moviepy - Writing video /home/meliioko/dqn-breakout/videos/rl-video-episode-0.mp4\n","\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 1/10000 [00:01<5:18:01,  1.91s/it]"]},{"name":"stdout","output_type":"stream","text":["Moviepy - Done !\n","Moviepy - video ready /home/meliioko/dqn-breakout/videos/rl-video-episode-0.mp4\n","Episode: 0/10000, Score: -20.0, Nb_frames : 1172\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 21/10000 [01:43<13:18:49,  4.80s/it]"]},{"name":"stdout","output_type":"stream","text":["Episode: 20/10000, Score: -19.0, Nb_frames : 26972\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 30/10000 [02:08<7:24:48,  2.68s/it] "]},{"name":"stdout","output_type":"stream","text":["Moviepy - Building video /home/meliioko/dqn-breakout/videos/rl-video-episode-30.mp4.\n","Moviepy - Writing video /home/meliioko/dqn-breakout/videos/rl-video-episode-30.mp4\n","\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 31/10000 [02:09<7:55:04,  2.86s/it]"]},{"name":"stdout","output_type":"stream","text":["Moviepy - Done !\n","Moviepy - video ready /home/meliioko/dqn-breakout/videos/rl-video-episode-30.mp4\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 35/10000 [02:19<7:44:34,  2.80s/it]"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["from tqdm import tqdm\n","\n","frames = 0\n","actions_q = []\n","rewards_all = []\n","# Training loop\n","for episode in tqdm(range(M)):\n","    total_reward = 0\n","    state = preprocess_state(env.reset()[0])# Add batch dimension\n","    for t in range(T):\n","        # Epsilon-greedy action selection\n","        if np.random.rand() <= epsilon:\n","            action = random.randrange(action_size)\n","        else:\n","            with torch.no_grad():  # No need to track gradients here\n","                act_values = Q(state)\n","                action = act_values.max(1)[1].item()  # Choose the action with the highest Q-value\n","                actions_q.append(action)\n","\n","        # Execute action in environment and observe next state and reward\n","        for i in range(4):\n","            next_state, reward, done, _, _ = env.step(action)\n","            frames += 1\n","            if i < 3:\n","              state = preprocess_state(next_state)\n","            total_reward += reward\n","\n","        next_state = preprocess_state(next_state)\n","\n","        # Store transition in D (experience replay buffer)\n","        D.append((state, action, reward, next_state, done))\n","\n","        state = next_state\n","\n","        # Check if the episode is done\n","        if done :\n","            if episode % 20 == 0:\n","              print(f\"Episode: {episode}/{M}, Score: {total_reward}, Nb_frames : {frames}\")\n","              rewards_all.append(total_reward)  \n","            break\n","\n","\n","\n","        # Train using a random minibatch from D\n","        if len(D) > 5000:\n","            minibatch = random.sample(D, 32)\n","            # Extract tensors from the minibatch\n","            states = torch.cat([s for s, a, r, ns, d in minibatch]).to(device)\n","            actions = torch.tensor([a for s, a, r, ns, d in minibatch], device=device).long()\n","            rewards = torch.tensor([r for s, a, r, ns, d in minibatch], device=device).float()\n","            next_states = torch.cat([ns for s, a, r, ns, d in minibatch]).to(device)\n","            dones = torch.tensor([d for s, a, r, ns, d in minibatch], device=device).float()\n","\n","\n","            # Compute Q values for current states\n","            Q_values = Q(states)\n","            # Select the Q value for the action taken, which are the ones we want to update\n","            Q_values = Q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n","\n","            # Compute the Q values for next states using the target network\n","            with torch.no_grad():\n","                next_state_values = Q_hat(next_states).max(1)[0]\n","                # If done is true, we want to ignore the next state value\n","                next_state_values[dones == 1] = 0.0\n","                # Compute the target Q values\n","                target_Q_values = rewards + (gamma * next_state_values)\n","\n","            # Zero the parameter gradients\n","            optimizer.zero_grad()\n","            # Compute loss\n","            loss = criterion(Q_values, target_Q_values)\n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","\n","    # Update epsilon\n","    if epsilon > epsilon_min:\n","        epsilon *= epsilon_decay\n","\n","    # Update target network\n","    if episode % C == 0:\n","        Q_hat.load_state_dict(Q.state_dict())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-04T12:34:34.420208Z","iopub.status.busy":"2023-11-04T12:34:34.419819Z","iopub.status.idle":"2023-11-04T12:34:34.443570Z","shell.execute_reply":"2023-11-04T12:34:34.442781Z","shell.execute_reply.started":"2023-11-04T12:34:34.420177Z"},"trusted":true},"outputs":[],"source":["np.save('actions.npy', np.asarray(actions_q))\n","torch.save(Q.state_dict(), 'q.pt')\n","saved_actions = np.save('rewards.npy', np.asarray(rewards_all))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}

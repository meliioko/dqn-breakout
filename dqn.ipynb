{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gym gym[other] tensorflow keras autorom gym[accept-rom-license]"
      ],
      "metadata": {
        "id": "hLvatxY5Wqp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1gQN86MWotC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3TBeAMFWotH",
        "outputId": "7eabe799-cb4d-4b44-cd9f-86dca0b763ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Breakout-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (210, 160)\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"Breakout-v0\", obs_type='grayscale', render_mode='rgb_array')\n",
        "env = gym.wrappers.AtariPreprocessing(env=env, frame_skip=1)\n",
        "env = gym.wrappers.FrameStack(env=env, num_stack=4)\n",
        "env = gym.wrappers.RecordVideo(env, '/content/videos', episode_trigger= lambda x : x % 10 == 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "336Q4MsWgtnu",
        "outputId": "261a8714-5e40-4308-81de-35572b6752f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)  # Assuming input_shape is (channels, height, width)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "\n",
        "        # Compute the size of the output of the last conv layer\n",
        "        def conv2d_size_out(size, kernel_size=3, stride=1):\n",
        "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
        "\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(84, 8, 4), 4, 2), 3, 1)\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(84, 8, 4), 4, 2), 3, 1)\n",
        "        linear_input_size = convw * convh * 64\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(linear_input_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "        return self.fc(x)\n",
        "\n",
        "def update_target_network(target, source):\n",
        "    target.load_state_dict(source.state_dict())\n"
      ],
      "metadata": {
        "id": "DAxcMkx5gsh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3TIpPwyWotJ"
      },
      "source": [
        "def preprocess_frame(frame):\n",
        "    # Resize, normalize, etc.\n",
        "    # ... your preprocessing steps here ...\n",
        "    processed_frame = np.resize(frame, (84, 84))  # This is a placeholder, replace with actual preprocessing\n",
        "    return processed_frame\n",
        "\n",
        "# Initialize frame stacker\n",
        "frame_stacker = deque(maxlen=4)\n",
        "\n",
        "# Function to stack frames\n",
        "def stack_frames(new_frame, is_new_episode=False, stack=frame_stacker):\n",
        "    frame = preprocess_frame(new_frame)\n",
        "    \n",
        "    if is_new_episode:\n",
        "        # Clear our stack\n",
        "        stack.clear()\n",
        "        # Because we're in a new episode, copy the same frame 4x\n",
        "        for _ in range(4):\n",
        "            stack.append(frame)\n",
        "    else:\n",
        "        # Append frame to deque, automatically removes the oldest frame\n",
        "        stack.append(frame)\n",
        "    \n",
        "    # Stack the frames along the third dimension and return a new numpy array\n",
        "    stacked_state = np.stack(stack, axis=2)\n",
        "    return stacked_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-9rxdLWWotK"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "# Set parameters\n",
        "N = 10000  # Replay memory capacity\n",
        "M = 10000  # Number of episodes\n",
        "T = 10000  # Max steps per episode\n",
        "C = 1000  # Target network update frequency\n",
        "epsilon = 1\n",
        "epsilon_decay = 0.995\n",
        "epsilon_min = 0.1\n",
        "gamma = 0.9\n",
        "action_size = env.action_space.n  # Number of actions\n",
        "state_size = env.observation_space.shape[0]  # State size\n",
        "\n",
        "# Initialize replay memory\n",
        "\n",
        "\n",
        "Q = DQN(action_size)\n",
        "Q_hat = copy.deepcopy(Q)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5deiK-qWotM",
        "outputId": "56f5ffb3-8563-4b1c-dc79-67d898e000ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(84, 84, 4)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "state = env.reset()\n",
        "next_state, reward, done, _, _ = env.step(1)\n",
        "np.asarray(next_state).reshape(84,84,4).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cf8kqTdyWotM"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Check if a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "Q.to(device)\n",
        "Q_hat.to(device)\n",
        "optimizer = optim.Adam(Q.parameters(), lr=0.01)\n",
        "criterion = nn.MSELoss()\n",
        "D = deque(maxlen=N)\n",
        "\n",
        "# Convert numpy array to PyTorch tensor\n",
        "def preprocess_state(state):\n",
        "  return torch.tensor(state).float().div(255).unsqueeze(0).to(device)  # Scales to [0,1]\n",
        "\n",
        "\n",
        "# Training loop\n",
        "for episode in tqdm(range(M)):\n",
        "    total_reward = 0\n",
        "    state = preprocess_state(env.reset()[0])# Add batch dimension\n",
        "    for t in range(T):\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.rand() <= epsilon:\n",
        "            action = random.randrange(action_size)\n",
        "        else:\n",
        "            with torch.no_grad():  # No need to track gradients here\n",
        "                act_values = Q(state)\n",
        "                action = act_values.max(1)[1].item()  # Choose the action with the highest Q-value\n",
        "\n",
        "        # Execute action in environment and observe next state and reward\n",
        "        for _ in range(4):\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "\n",
        "        next_state = preprocess_state(next_state)\n",
        "\n",
        "        # Store transition in D (experience replay buffer)\n",
        "        D.append((state, action, reward, next_state, done))\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # Check if the episode is done\n",
        "        if done :\n",
        "            #if episode % 100 == 0:\n",
        "            print(f\"Episode: {episode}/{M}, Score: {total_reward}\")\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "        # Train using a random minibatch from D\n",
        "        if len(D) > 32:\n",
        "            minibatch = random.sample(D, 32)\n",
        "            # Extract tensors from the minibatch\n",
        "            states = torch.cat([s for s, a, r, ns, d in minibatch]).to(device)\n",
        "            actions = torch.tensor([a for s, a, r, ns, d in minibatch], device=device).long()\n",
        "            rewards = torch.tensor([r for s, a, r, ns, d in minibatch], device=device).float()\n",
        "            next_states = torch.cat([ns for s, a, r, ns, d in minibatch]).to(device)\n",
        "            dones = torch.tensor([d for s, a, r, ns, d in minibatch], device=device).float()\n",
        "\n",
        "\n",
        "            # Compute Q values for current states\n",
        "            Q_values = Q(states)\n",
        "            # Select the Q value for the action taken, which are the ones we want to update\n",
        "            Q_values = Q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Compute the Q values for next states using the target network\n",
        "            with torch.no_grad():\n",
        "                next_state_values = Q_hat(next_states).max(1)[0]\n",
        "                # If done is true, we want to ignore the next state value\n",
        "                next_state_values[dones == 1] = 0.0\n",
        "                # Compute the target Q values\n",
        "                target_Q_values = rewards + (gamma * next_state_values)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Compute loss\n",
        "            loss = criterion(Q_values, target_Q_values)\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Update epsilon\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    # Update target network\n",
        "    if episode % C == 0:\n",
        "        Q_hat.load_state_dict(Q.state_dict())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
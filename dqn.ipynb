{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gym gym[other] tensorflow keras autorom gym[accept-rom-license] gym[atari] torch","metadata":{"id":"hLvatxY5Wqp8","outputId":"e9ffed7a-fef0-43e7-9adc-e2030654ee91","execution":{"iopub.status.busy":"2023-11-04T12:20:10.464994Z","iopub.execute_input":"2023-11-04T12:20:10.465396Z","iopub.status.idle":"2023-11-04T12:20:22.670564Z","shell.execute_reply.started":"2023-11-04T12:20:10.465363Z","shell.execute_reply":"2023-11-04T12:20:22.669338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom collections import deque\nimport gym\nimport random","metadata":{"id":"G1gQN86MWotC","execution":{"iopub.status.busy":"2023-11-04T12:20:22.673289Z","iopub.execute_input":"2023-11-04T12:20:22.673681Z","iopub.status.idle":"2023-11-04T12:20:22.678717Z","shell.execute_reply.started":"2023-11-04T12:20:22.673641Z","shell.execute_reply":"2023-11-04T12:20:22.677771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = gym.make(\"Breakout-v0\", obs_type='grayscale', render_mode='rgb_array')\nenv = gym.wrappers.AtariPreprocessing(env=env, frame_skip=1)\nenv = gym.wrappers.FrameStack(env=env, num_stack=4)\n# env = gym.wrappers.RecordVideo(env, '/kaggle/output/videos', episode_trigger= lambda x : x % 100 == 0)\n","metadata":{"id":"a3TBeAMFWotH","outputId":"2edc9e00-7fc3-4fa1-8003-b0d62ecf8d37","execution":{"iopub.status.busy":"2023-11-04T12:20:22.679923Z","iopub.execute_input":"2023-11-04T12:20:22.680207Z","iopub.status.idle":"2023-11-04T12:20:22.874992Z","shell.execute_reply.started":"2023-11-04T12:20:22.680183Z","shell.execute_reply":"2023-11-04T12:20:22.874041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass DQN(nn.Module):\n    def __init__(self, action_size):\n        super(DQN, self).__init__()\n        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)  # Assuming input_shape is (channels, height, width)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n\n        # Compute the size of the output of the last conv layer\n        def conv2d_size_out(size, kernel_size=3, stride=1):\n            return (size - (kernel_size - 1) - 1) // stride + 1\n\n        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(84, 8, 4), 4, 2), 3, 1)\n        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(84, 8, 4), 4, 2), 3, 1)\n        linear_input_size = convw * convh * 64\n\n        self.fc = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(linear_input_size, 512),\n            nn.ReLU(),\n            nn.Linear(512, action_size)\n        )\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.relu(self.conv2(x))\n        x = torch.relu(self.conv3(x))\n        return self.fc(x)\n\ndef update_target_network(target, source):\n    target.load_state_dict(source.state_dict())\n","metadata":{"id":"DAxcMkx5gsh2","execution":{"iopub.status.busy":"2023-11-04T12:20:22.876596Z","iopub.execute_input":"2023-11-04T12:20:22.877289Z","iopub.status.idle":"2023-11-04T12:20:22.889044Z","shell.execute_reply.started":"2023-11-04T12:20:22.877251Z","shell.execute_reply":"2023-11-04T12:20:22.888087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import copy\n# Set parameters\nN = 60000  # Replay memory capacity\nM = 5000  # Number of episodes\nT = 10000  # Max steps per episode\nC = 40  # Target network update frequency\nepsilon = 1\nepsilon_decay = 0.99\nepsilon_min = 0.1\ngamma = 0.9\naction_size = env.action_space.n  # Number of actions\nstate_size = env.observation_space.shape[0]  # State size\n\n# Initialize replay memory\n\n\nQ = DQN(action_size)\nQ_hat = copy.deepcopy(Q)\n\n\n","metadata":{"id":"X-9rxdLWWotK","execution":{"iopub.status.busy":"2023-11-04T12:20:22.893235Z","iopub.execute_input":"2023-11-04T12:20:22.893786Z","iopub.status.idle":"2023-11-04T12:20:22.922299Z","shell.execute_reply.started":"2023-11-04T12:20:22.893751Z","shell.execute_reply":"2023-11-04T12:20:22.921464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Check if a GPU is available\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using GPU:\", torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU\")\n\nQ.to(device)\nQ_hat.to(device)\noptimizer = optim.Adam(Q.parameters(), lr=0.00025)\ncriterion = nn.MSELoss()\nD = deque(maxlen=N)\n\n# Convert numpy array to PyTorch tensor\ndef preprocess_state(state):\n  return torch.tensor(np.asarray(state)).float().div(255).unsqueeze(0).to(device)  # Scales to [0,1]\n\nframes = 0\nactions_q = []\nrewards_all = []\n# Training loop\nfor episode in tqdm(range(M)):\n    total_reward = 0\n    state = preprocess_state(env.reset()[0])# Add batch dimension\n    for t in range(T):\n        # Epsilon-greedy action selection\n        if np.random.rand() <= epsilon:\n            action = random.randrange(action_size)\n        else:\n            with torch.no_grad():  # No need to track gradients here\n                act_values = Q(state)\n                action = act_values.max(1)[1].item()  # Choose the action with the highest Q-value\n                actions_q.append(action)\n\n        # Execute action in environment and observe next state and reward\n        for i in range(4):\n            next_state, reward, done, _, _ = env.step(action)\n            frames += 1\n            if i < 3:\n              state = preprocess_state(next_state)\n            total_reward += reward\n\n        next_state = preprocess_state(next_state)\n\n        # Store transition in D (experience replay buffer)\n        D.append((state, action, reward, next_state, done))\n\n        state = next_state\n\n        # Check if the episode is done\n        if done :\n            if episode % 20 == 0:\n              print(f\"Episode: {episode}/{M}, Score: {total_reward}, Nb_frames : {frames}\")\n              rewards_all.append(total_reward)  \n            break\n\n\n\n        # Train using a random minibatch from D\n        if len(D) > 5000:\n            minibatch = random.sample(D, 32)\n            # Extract tensors from the minibatch\n            states = torch.cat([s for s, a, r, ns, d in minibatch]).to(device)\n            actions = torch.tensor([a for s, a, r, ns, d in minibatch], device=device).long()\n            rewards = torch.tensor([r for s, a, r, ns, d in minibatch], device=device).float()\n            next_states = torch.cat([ns for s, a, r, ns, d in minibatch]).to(device)\n            dones = torch.tensor([d for s, a, r, ns, d in minibatch], device=device).float()\n\n\n            # Compute Q values for current states\n            Q_values = Q(states)\n            # Select the Q value for the action taken, which are the ones we want to update\n            Q_values = Q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n\n            # Compute the Q values for next states using the target network\n            with torch.no_grad():\n                next_state_values = Q_hat(next_states).max(1)[0]\n                # If done is true, we want to ignore the next state value\n                next_state_values[dones == 1] = 0.0\n                # Compute the target Q values\n                target_Q_values = rewards + (gamma * next_state_values)\n\n            # Zero the parameter gradients\n            optimizer.zero_grad()\n            # Compute loss\n            loss = criterion(Q_values, target_Q_values)\n            # Backward pass\n            loss.backward()\n            optimizer.step()\n\n    # Update epsilon\n    if epsilon > epsilon_min:\n        epsilon *= epsilon_decay\n\n    # Update target network\n    if episode % C == 0:\n        Q_hat.load_state_dict(Q.state_dict())","metadata":{"id":"Cf8kqTdyWotM","outputId":"77df7ede-d0e7-4470-a7a2-710ab31504ad","execution":{"iopub.status.busy":"2023-11-04T12:25:19.997279Z","iopub.execute_input":"2023-11-04T12:25:19.997666Z","iopub.status.idle":"2023-11-04T12:27:05.158790Z","shell.execute_reply.started":"2023-11-04T12:25:19.997626Z","shell.execute_reply":"2023-11-04T12:27:05.157415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('actions.npy', np.asarray(actions_q))\ntorch.save(Q.state_dict(), 'q.pt')\nsaved_actions = np.save('rewards.npy', np.asarray(rewards_all))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-04T12:34:34.419819Z","iopub.execute_input":"2023-11-04T12:34:34.420208Z","iopub.status.idle":"2023-11-04T12:34:34.443570Z","shell.execute_reply.started":"2023-11-04T12:34:34.420177Z","shell.execute_reply":"2023-11-04T12:34:34.442781Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T12:20:10.465396Z",
          "iopub.status.busy": "2023-11-04T12:20:10.464994Z",
          "iopub.status.idle": "2023-11-04T12:20:22.670564Z",
          "shell.execute_reply": "2023-11-04T12:20:22.669338Z",
          "shell.execute_reply.started": "2023-11-04T12:20:10.465363Z"
        },
        "id": "hLvatxY5Wqp8",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install -U gym gym[other] tensorflow keras autorom gym[accept-rom-license] gym[atari] torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T12:20:22.673681Z",
          "iopub.status.busy": "2023-11-04T12:20:22.673289Z",
          "iopub.status.idle": "2023-11-04T12:20:22.678717Z",
          "shell.execute_reply": "2023-11-04T12:20:22.677771Z",
          "shell.execute_reply.started": "2023-11-04T12:20:22.673641Z"
        },
        "id": "G1gQN86MWotC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "import gym\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T12:20:22.680207Z",
          "iopub.status.busy": "2023-11-04T12:20:22.679923Z",
          "iopub.status.idle": "2023-11-04T12:20:22.874992Z",
          "shell.execute_reply": "2023-11-04T12:20:22.874041Z",
          "shell.execute_reply.started": "2023-11-04T12:20:22.680183Z"
        },
        "id": "a3TBeAMFWotH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Pong-v4\", obs_type='grayscale', render_mode='rgb_array', full_action_space=False, frameskip=4)\n",
        "env = gym.wrappers.AtariPreprocessing(env=env, frame_skip=1, terminal_on_life_loss=True)\n",
        "env = gym.wrappers.FrameStack(env=env, num_stack=4)\n",
        "# env = gym.wrappers.RecordVideo(env, 'videos', episode_trigger= lambda x : x % 30 == 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "state"
      ],
      "metadata": {
        "id": "NvzHUTtMWVMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T12:20:22.877289Z",
          "iopub.status.busy": "2023-11-04T12:20:22.876596Z",
          "iopub.status.idle": "2023-11-04T12:20:22.889044Z",
          "shell.execute_reply": "2023-11-04T12:20:22.888087Z",
          "shell.execute_reply.started": "2023-11-04T12:20:22.877251Z"
        },
        "id": "DAxcMkx5gsh2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)  # Assuming input_shape is (channels, height, width)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "\n",
        "        # Compute the size of the output of the last conv layer\n",
        "        def conv2d_size_out(size, kernel_size=3, stride=1):\n",
        "            return (size - (kernel_size - 1) - 1) // stride + 1\n",
        "\n",
        "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(84, 8, 4), 4, 2), 3, 1)\n",
        "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(84, 8, 4), 4, 2), 3, 1)\n",
        "        linear_input_size = convw * convh * 64\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(linear_input_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, action_size)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = torch.relu(self.conv1(x))\n",
        "        x = torch.relu(self.conv2(x))\n",
        "        x = torch.relu(self.conv3(x))\n",
        "\n",
        "        return self.fc(x)\n",
        "\n",
        "def update_target_network(target, source):\n",
        "    target.load_state_dict(source.state_dict())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T12:20:22.893786Z",
          "iopub.status.busy": "2023-11-04T12:20:22.893235Z",
          "iopub.status.idle": "2023-11-04T12:20:22.922299Z",
          "shell.execute_reply": "2023-11-04T12:20:22.921464Z",
          "shell.execute_reply.started": "2023-11-04T12:20:22.893751Z"
        },
        "id": "X-9rxdLWWotK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "# Set parameters\n",
        "N = 40000  # Replay memory capacity\n",
        "M = 10000  # Number of episodes\n",
        "T = 10000  # Max steps per episode\n",
        "C = 40  # Target network update frequency\n",
        "epsilon = 1\n",
        "epsilon_decay = 0.99\n",
        "epsilon_min = 0.1\n",
        "gamma = 0.99\n",
        "action_size = env.action_space.n  # Number of actions\n",
        "state_size = env.observation_space.shape[0]  # State size\n",
        "\n",
        "# Initialize replay memory\n",
        "\n",
        "\n",
        "Q = DQN(action_size)\n",
        "Q_hat = copy.deepcopy(Q)\n",
        "D = deque(maxlen=N)\n",
        "\n",
        "\n",
        "# Check if a GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "Q.to(device)\n",
        "Q_hat.to(device)\n",
        "optimizer = optim.Adam(Q.parameters(), lr=0.0025)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "\n",
        "# Convert numpy array to PyTorch tensor\n",
        "def preprocess_state(state):\n",
        "  return torch.tensor(np.asarray(state)).float().div(255).unsqueeze(0).to(device)  # Scales to [0,1]\n",
        "\n",
        "frames = 0\n",
        "reward_list = []\n",
        "loss_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-04T12:25:19.997666Z",
          "iopub.status.busy": "2023-11-04T12:25:19.997279Z",
          "iopub.status.idle": "2023-11-04T12:27:05.158790Z",
          "shell.execute_reply": "2023-11-04T12:27:05.157415Z",
          "shell.execute_reply.started": "2023-11-04T12:25:19.997626Z"
        },
        "id": "Cf8kqTdyWotM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Training loop\n",
        "for episode in tqdm(range(2000)):\n",
        "    total_reward = 0\n",
        "    state = preprocess_state(env.reset()[0])# Add batch dimension\n",
        "    for t in range(T):\n",
        "        # Epsilon-greedy action selection\n",
        "        if np.random.rand() <= epsilon:\n",
        "            action = random.randrange(action_size)\n",
        "        else:\n",
        "            with torch.no_grad():  # No need to track gradients here\n",
        "                act_values = Q(state)\n",
        "                action = act_values.max(1)[1].item()  # Choose the action with the highest Q-value\n",
        "\n",
        "        # Execute action in environment and observe next state and reward\n",
        "        next_state, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        frames += 1\n",
        "\n",
        "        next_state = preprocess_state(next_state)\n",
        "\n",
        "        # Store transition in D (experience replay buffer)\n",
        "        D.append((state, action, reward, next_state, done))\n",
        "        d_len = len(D)\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        # Check if the episode is done\n",
        "        if done :\n",
        "            if episode % 20 == 0:\n",
        "              print(f\"Episode: {episode}/{M}, Score: {total_reward}, Nb_frames : {frames}\")\n",
        "            reward_list.append(total_reward)\n",
        "            break\n",
        "\n",
        "\n",
        "\n",
        "        # Train using a random minibatch from D\n",
        "        if len(D) > 5000:\n",
        "            minibatch = random.sample(D, 32)\n",
        "            # Extract tensors from the minibatch\n",
        "            states = torch.cat([s for s, a, r, ns, d in minibatch]).to(device)\n",
        "            actions = torch.tensor([a for s, a, r, ns, d in minibatch], device=device).long()\n",
        "            rewards = torch.tensor([r for s, a, r, ns, d in minibatch], device=device).float()\n",
        "            next_states = torch.cat([ns for s, a, r, ns, d in minibatch]).to(device)\n",
        "            dones = torch.tensor([d for s, a, r, ns, d in minibatch], device=device).float()\n",
        "\n",
        "\n",
        "            # Compute Q values for current states\n",
        "            Q_values = Q(states)\n",
        "            # Select the Q value for the action taken, which are the ones we want to update\n",
        "            Q_values = Q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Compute the Q values for next states using the target network\n",
        "            with torch.no_grad():\n",
        "                next_state_values = Q_hat(next_states).max(1)[0]\n",
        "                # If done is true, we want to ignore the next state value\n",
        "                next_state_values[dones == 1] = 0.0\n",
        "                # Compute the target Q values\n",
        "                target_Q_values = rewards + (gamma * next_state_values)\n",
        "\n",
        "            # Zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Compute loss\n",
        "            loss = criterion(Q_values, target_Q_values)\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            loss_list.append(loss.item())\n",
        "            torch.nn.utils.clip_grad_norm_(Q.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "    # Update epsilon\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "    # Update target network\n",
        "    if episode % C == 0:\n",
        "        Q_hat.load_state_dict(Q.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(loss_list)"
      ],
      "metadata": {
        "id": "nIh3t5VMdVRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(reward_list)"
      ],
      "metadata": {
        "id": "MoTBId8WdXtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the linear model\n",
        "x = np.arange(1, len(reward_list) + 1)\n",
        "y = reward_list\n",
        "coefficients = np.polyfit(x, y, 1)  # 1 means linear\n",
        "polynomial = np.poly1d(coefficients)\n",
        "\n",
        "# Generate a number of points for the x-axis (from min to max of your original x)\n",
        "x_trend = np.linspace(x.min(), x.max(), 100)\n",
        "# Predict the y values\n",
        "y_trend = polynomial(x_trend)\n",
        "\n",
        "# Plot the linear trend\n",
        "plt.plot(x_trend, y_trend, label='Linear Trend', color='red')\n",
        "\n",
        "# Optionally, if you want to plot the original data points:\n",
        "plt.scatter(x, y, label='Data Points')\n",
        "\n",
        "plt.xlabel('X-axis')\n",
        "plt.ylabel('Y-axis')\n",
        "plt.title('Linear Trend of Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DKQXmF7yeWZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = preprocess_state(env.reset()[0])# Add batch dimension\n",
        "total_reward = 0\n",
        "q_values = []\n",
        "while(True):\n",
        "  with torch.no_grad():  # No need to track gradients here\n",
        "    act_values = Q(state)\n",
        "    action = act_values.max(1)[1].item()  # Choose the action with the highest Q-value\n",
        "    q_values.append(act_values.to('cpu').numpy())\n",
        "    next_state, reward, done, _, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    if done:\n",
        "      break\n",
        "    state = preprocess_state(next_state)\n",
        "print(total_reward)"
      ],
      "metadata": {
        "id": "e8bubS97RbdB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
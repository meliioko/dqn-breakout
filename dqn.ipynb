{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-02 11:11:28.516519: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-02 11:11:28.604103: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-02 11:11:29.017816: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-02 11:11:29.017867: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-02 11:11:29.019905: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-02 11:11:29.198632: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-02 11:11:29.201115: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-02 11:11:31.372551: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym\n",
    "from tensorflow.keras import models, layers\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/meliioko/atari/.venv/lib/python3.10/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Breakout-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n",
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n",
      "/home/meliioko/atari/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:31: UserWarning: \u001b[33mWARN: A Box observation space has an unconventional shape (neither an image, nor a 1D vector). We recommend flattening the observation to have only a 1D vector or use a custom policy to properly process the data. Actual observation shape: (210, 160)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Breakout-v0\", obs_type='grayscale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(action_size):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(84, 84, 4)))\n",
    "    model.add(layers.Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_state(state):\n",
    "    # Assuming the state is already grayscale\n",
    "    # Resize to 84x84 and reshape to the expected input shape for the model\n",
    "    processed_state = resize(state, (84, 84)).reshape(1, 84, 84, 1)\n",
    "    # Stack 4 frames\n",
    "    processed_state = np.repeat(processed_state, 4, axis=3)\n",
    "    return processed_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Convert frame to grayscale if it is not already (assuming the environment returns colored frames)\n",
    "    # Resize, normalize, etc.\n",
    "    # ... your preprocessing steps here ...\n",
    "    processed_frame = resize(frame, (84, 84))  # This is a placeholder, replace with actual preprocessing\n",
    "    return processed_frame\n",
    "\n",
    "# Initialize frame stacker\n",
    "frame_stacker = deque(maxlen=4)\n",
    "\n",
    "# Function to stack frames\n",
    "def stack_frames(stack, new_frame, is_new_episode):\n",
    "    frame = preprocess_frame(new_frame)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stack\n",
    "        stack.clear()\n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        for _ in range(4):\n",
    "            stack.append(frame)\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stack.append(frame)\n",
    "    \n",
    "    # Stack the frames along the third dimension and return a new numpy array\n",
    "    stacked_state = np.stack(stack, axis=2)\n",
    "    return stacked_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "N = 10000  # Replay memory capacity\n",
    "M = 1000  # Number of episodes\n",
    "T = 10000  # Max steps per episode\n",
    "C = 1000  # Target network update frequency\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.1\n",
    "gamma = 0.9\n",
    "action_size = env.action_space.n  # Number of actions\n",
    "state_size = env.observation_space.shape[0]  # State size\n",
    "\n",
    "# Initialize replay memory\n",
    "D = deque(maxlen=N)\n",
    "\n",
    "\n",
    "Q = build_model(action_size)\n",
    "Q_hat = build_model(action_size)\n",
    "Q_hat.set_weights(Q.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1686180 (6.43 MB)\n",
      "Trainable params: 1686180 (6.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Q.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/meliioko/atari/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2416, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/meliioko/atari/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2401, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/meliioko/atari/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2389, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/meliioko/atari/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2357, in predict_step\n        return self(x, training=False)\n    File \"/home/meliioko/atari/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/meliioko/atari/.venv/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_6\" is incompatible with the layer: expected shape=(None, 84, 84, 4), found shape=(None, 160)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/meliioko/atari/dqn.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/meliioko/atari/dqn.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m target \u001b[39m=\u001b[39m r\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/meliioko/atari/dqn.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m terminal:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/meliioko/atari/dqn.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     target \u001b[39m=\u001b[39m (r \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mamax(Q_hat\u001b[39m.\u001b[39;49mpredict(w_next)[\u001b[39m0\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/meliioko/atari/dqn.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m target_f \u001b[39m=\u001b[39m Q\u001b[39m.\u001b[39mpredict(w)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/meliioko/atari/dqn.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m target_f[\u001b[39m0\u001b[39m][a] \u001b[39m=\u001b[39m target\n",
      "File \u001b[0;32m~/atari/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filep6c_w7s8.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/meliioko/atari/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2416, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/meliioko/atari/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2401, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/meliioko/atari/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2389, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/meliioko/atari/.venv/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2357, in predict_step\n        return self(x, training=False)\n    File \"/home/meliioko/atari/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/home/meliioko/atari/.venv/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_6\" is incompatible with the layer: expected shape=(None, 84, 84, 4), found shape=(None, 160)\n"
     ]
    }
   ],
   "source": [
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for t in range(T):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() <= epsilon:\n",
    "            action = random.randrange(action_size)\n",
    "        else:\n",
    "            act_values = Q.predict(state)\n",
    "            action = np.argmax(act_values[0])\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Store transition in D\n",
    "        D.append((state, action, reward, next_state, done))\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        # Check if the episode is done\n",
    "        if done:\n",
    "            print(f\"Episode: {episode}/{M}, Score: {total_reward}\")\n",
    "            break\n",
    "\n",
    "        # Train using a random minibatch from D\n",
    "        if len(D) > 32:\n",
    "            minibatch = random.sample(D, 32)\n",
    "            for w, a, r, w_next, terminal in minibatch:\n",
    "                target = r\n",
    "                if not terminal:\n",
    "                    target = (r + gamma * np.amax(Q_hat.predict(w_next)[0]))\n",
    "                target_f = Q.predict(w)\n",
    "                target_f[0][a] = target\n",
    "                Q.fit(w, target_f, epochs=1, verbose=0)\n",
    "\n",
    "    # Update epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay\n",
    "\n",
    "    # Update target network\n",
    "    if episode % C == 0:\n",
    "        Q_hat.set_weights(Q.get_weights())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

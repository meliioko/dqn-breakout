{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 09:13:15.121924: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-03 09:13:15.126914: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-03 09:13:15.198406: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-03 09:13:15.198443: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-03 09:13:15.198462: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-03 09:13:15.206245: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-03 09:13:15.208507: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-03 09:13:16.337393: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym\n",
    "from tensorflow.keras import models, layers\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Breakout-v0\", obs_type='grayscale')\n",
    "env = gym.wrappers.AtariPreprocessing(env=env, frame_skip=1)\n",
    "env = gym.wrappers.FrameStack(env=env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(action_size):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=(84, 84, 4)))\n",
    "    model.add(layers.Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def preprocess_frame(frame):\n",
    "    # Resize, normalize, etc.\n",
    "    # ... your preprocessing steps here ...\n",
    "    processed_frame = np.resize(frame, (84, 84))  # This is a placeholder, replace with actual preprocessing\n",
    "    return processed_frame\n",
    "\n",
    "# Initialize frame stacker\n",
    "frame_stacker = deque(maxlen=4)\n",
    "\n",
    "# Function to stack frames\n",
    "def stack_frames(new_frame, is_new_episode=False, stack=frame_stacker):\n",
    "    frame = preprocess_frame(new_frame)\n",
    "    \n",
    "    if is_new_episode:\n",
    "        # Clear our stack\n",
    "        stack.clear()\n",
    "        # Because we're in a new episode, copy the same frame 4x\n",
    "        for _ in range(4):\n",
    "            stack.append(frame)\n",
    "    else:\n",
    "        # Append frame to deque, automatically removes the oldest frame\n",
    "        stack.append(frame)\n",
    "    \n",
    "    # Stack the frames along the third dimension and return a new numpy array\n",
    "    stacked_state = np.stack(stack, axis=2)\n",
    "    return stacked_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "N = 10000  # Replay memory capacity\n",
    "M = 1000  # Number of episodes\n",
    "T = 10000  # Max steps per episode\n",
    "C = 1000  # Target network update frequency\n",
    "epsilon = 1\n",
    "epsilon_decay = 0.995\n",
    "epsilon_min = 0.1\n",
    "gamma = 0.9\n",
    "action_size = env.action_space.n  # Number of actions\n",
    "state_size = env.observation_space.shape[0]  # State size\n",
    "\n",
    "# Initialize replay memory\n",
    "D = deque(maxlen=N)\n",
    "\n",
    "\n",
    "Q = build_model(action_size)\n",
    "Q_hat = build_model(action_size)\n",
    "Q_hat.set_weights(Q.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 3136)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1686180 (6.43 MB)\n",
      "Trainable params: 1686180 (6.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Q.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 20, 20, 32)        8224      \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 9, 9, 64)          32832     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 7, 7, 64)          36928     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1606144   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 2052      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1686180 (6.43 MB)\n",
      "Trainable params: 1686180 (6.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Q_hat.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84, 84, 4)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "next_state, reward, done, _, _ = env.step(1)\n",
    "np.asarray(next_state).reshape(84,84,4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/10000 [00:55<22:09:21,  7.98s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/meliioko/atari/dqn.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/meliioko/atari/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m terminal:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/meliioko/atari/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     target \u001b[39m=\u001b[39m (reward \u001b[39m+\u001b[39m gamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mamax(Q_hat\u001b[39m.\u001b[39mpredict(np\u001b[39m.\u001b[39mexpand_dims(w_next, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)))\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/meliioko/atari/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m target_f \u001b[39m=\u001b[39m Q\u001b[39m.\u001b[39;49mpredict(np\u001b[39m.\u001b[39;49mexpand_dims(w, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m), verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/meliioko/atari/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m target_f[\u001b[39m0\u001b[39m][a] \u001b[39m=\u001b[39m target\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/meliioko/atari/dqn.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m Q\u001b[39m.\u001b[39mfit(np\u001b[39m.\u001b[39mexpand_dims(w, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), target_f, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/atari/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/atari/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:2596\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2587\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m   2588\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   2589\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2590\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2593\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[1;32m   2594\u001b[0m         )\n\u001b[0;32m-> 2596\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[1;32m   2597\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m   2598\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   2599\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m   2600\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m   2601\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m   2602\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   2603\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   2604\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   2605\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2606\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[1;32m   2607\u001b[0m )\n\u001b[1;32m   2609\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   2610\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/atari/.venv/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1688\u001b[0m, in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1687\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1688\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/atari/.venv/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:1292\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1289\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[1;32m   1291\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[1;32m   1293\u001b[0m     x,\n\u001b[1;32m   1294\u001b[0m     y,\n\u001b[1;32m   1295\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m   1296\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m   1297\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[1;32m   1298\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m   1299\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m   1300\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m   1301\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m   1302\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m   1303\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[1;32m   1304\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1305\u001b[0m     pss_evaluation_shards\u001b[39m=\u001b[39;49mpss_evaluation_shards,\n\u001b[1;32m   1306\u001b[0m )\n\u001b[1;32m   1308\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[1;32m   1310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/atari/.venv/lib/python3.10/site-packages/keras/src/engine/data_adapter.py:314\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[39mreturn\u001b[39;00m indices\n\u001b[1;32m    309\u001b[0m \u001b[39m# We prefetch a single element. Computing large permutations can take\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39m# quite a while so we don't want to wait for prefetching over an epoch\u001b[39;00m\n\u001b[1;32m    311\u001b[0m \u001b[39m# boundary to trigger the next permutation. On the other hand, too many\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[39m# simultaneous shuffles can contend on a hardware level and degrade all\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m# performance.\u001b[39;00m\n\u001b[0;32m--> 314\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mmap(permutation)\u001b[39m.\u001b[39mprefetch(\u001b[39m1\u001b[39m)\n\u001b[1;32m    316\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mslice_batch_indices\u001b[39m(indices):\n\u001b[1;32m    317\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \n\u001b[1;32m    319\u001b[0m \u001b[39m    This step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[39m      A Dataset of batched indices.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/atari/.venv/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2268\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2264\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[1;32m   2265\u001b[0m \u001b[39m# dataset_ops).\u001b[39;00m\n\u001b[1;32m   2266\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m map_op\n\u001b[0;32m-> 2268\u001b[0m \u001b[39mreturn\u001b[39;00m map_op\u001b[39m.\u001b[39;49m_map_v2(\n\u001b[1;32m   2269\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2270\u001b[0m     map_func,\n\u001b[1;32m   2271\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39;49mnum_parallel_calls,\n\u001b[1;32m   2272\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m   2273\u001b[0m     name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/atari/.venv/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py:37\u001b[0m, in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m debug_mode\u001b[39m.\u001b[39mDEBUG_MODE:\n\u001b[1;32m     35\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mThe `deterministic` argument has no effect unless the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39m`num_parallel_calls` argument is specified.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m   \u001b[39mreturn\u001b[39;00m _MapDataset(\n\u001b[1;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m   \u001b[39mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[1;32m     41\u001b[0m       input_dataset,\n\u001b[1;32m     42\u001b[0m       map_func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     46\u001b[0m       name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/atari/.venv/lib/python3.10/site-packages/tensorflow/python/data/ops/map_op.py:113\u001b[0m, in \u001b[0;36m_MapDataset.__init__\u001b[0;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39mStructuredFunctionWrapper(\n\u001b[1;32m    108\u001b[0m     map_func,\n\u001b[1;32m    109\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transformation_name(),\n\u001b[1;32m    110\u001b[0m     dataset\u001b[39m=\u001b[39minput_dataset,\n\u001b[1;32m    111\u001b[0m     use_legacy_function\u001b[39m=\u001b[39muse_legacy_function)\n\u001b[1;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m name\n\u001b[0;32m--> 113\u001b[0m variant_tensor \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49mmap_dataset(\n\u001b[1;32m    114\u001b[0m     input_dataset\u001b[39m.\u001b[39;49m_variant_tensor,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    115\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_func\u001b[39m.\u001b[39;49mfunction\u001b[39m.\u001b[39;49mcaptured_inputs,\n\u001b[1;32m    116\u001b[0m     f\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_func\u001b[39m.\u001b[39;49mfunction,\n\u001b[1;32m    117\u001b[0m     use_inter_op_parallelism\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_use_inter_op_parallelism,\n\u001b[1;32m    118\u001b[0m     preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_preserve_cardinality,\n\u001b[1;32m    119\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_common_args)\n\u001b[1;32m    120\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(input_dataset, variant_tensor)\n",
      "File \u001b[0;32m~/atari/.venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3471\u001b[0m, in \u001b[0;36mmap_dataset\u001b[0;34m(input_dataset, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, preserve_cardinality, metadata, name)\u001b[0m\n\u001b[1;32m   3469\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3470\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3471\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3472\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMapDataset\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, input_dataset, other_arguments, \u001b[39m\"\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m, f,\n\u001b[1;32m   3473\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types, \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes,\n\u001b[1;32m   3474\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39muse_inter_op_parallelism\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_inter_op_parallelism,\n\u001b[1;32m   3475\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mpreserve_cardinality\u001b[39;49m\u001b[39m\"\u001b[39;49m, preserve_cardinality, \u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m, metadata)\n\u001b[1;32m   3476\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3477\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "state = np.asarray(env.reset()[0]).reshape(84,84,4)\n",
    "total_reward = 0\n",
    "for t in tqdm(range(T)):\n",
    "    # Epsilon-greedy action selection\n",
    "    if np.random.rand() <= epsilon:\n",
    "        action = random.randrange(action_size)\n",
    "    else:\n",
    "        act_values = Q.predict(state)\n",
    "        action = np.argmax(act_values[0])\n",
    "\n",
    "    next_state, reward, done, _, _ = env.step(action)\n",
    "    next_state = np.asarray(next_state).reshape(84,84,4)\n",
    "    total_reward += reward\n",
    "\n",
    "    # Store transition in D\n",
    "    D.append((state, action, reward, next_state, done))\n",
    "\n",
    "    state = next_state\n",
    "\n",
    "    # Check if the episode is done\n",
    "    if done:\n",
    "        print(f\"Episode: {episode}/{M}, Score: {total_reward}\")\n",
    "        break\n",
    "\n",
    "    # Train using a random minibatch from D\n",
    "    if len(D) > 32:\n",
    "        minibatch = random.sample(D, 32)\n",
    "        for w, a, r, w_next, terminal in minibatch:\n",
    "            target = r\n",
    "            if not terminal:\n",
    "                target = (reward + gamma * np.amax(Q_hat.predict(np.expand_dims(w_next, axis=0), verbose=0)))\n",
    "\n",
    "            target_f = Q.predict(np.expand_dims(w, axis=0), verbose=0)\n",
    "            target_f[0][a] = target\n",
    "            Q.fit(np.expand_dims(w, axis=0), target_f, epochs=1, verbose=0)\n",
    "\n",
    "# Update epsilon\n",
    "if epsilon > epsilon_min:\n",
    "    epsilon *= epsilon_decay\n",
    "\n",
    "# Update target network\n",
    "if episode % C == 0:\n",
    "    Q_hat.set_weights(Q.get_weights())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
